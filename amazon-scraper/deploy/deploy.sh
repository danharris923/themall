#!/bin/bash
# Deploy Amazon scraper to DigitalOcean droplet
# Run this from your LOCAL Ubuntu machine

set -e  # Exit on error

# Configuration
DROPLET_IP="146.190.240.167"
DROPLET_USER="root"
PROJECT_DIR="~/themall"
LOCAL_DIR="/home/ren/Desktop/themall"

echo "üöÄ Deploying Amazon Scraper to DigitalOcean..."
echo "üìç Target: $DROPLET_USER@$DROPLET_IP"
echo ""

# Check if we can connect
echo "üîç Testing SSH connection..."
if ! ssh -o ConnectTimeout=5 -o BatchMode=yes $DROPLET_USER@$DROPLET_IP exit 2>/dev/null; then
    echo "‚ùå Cannot connect via SSH key"
    echo "üí° You'll need to enter password for each command"
    echo ""
fi

# Sync code to droplet
echo "üì¶ Syncing code to droplet..."
rsync -avz --progress \
    --exclude='venv/' \
    --exclude='data/' \
    --exclude='logs/' \
    --exclude='__pycache__/' \
    --exclude='*.pyc' \
    --exclude='.git/' \
    --exclude='DROPLET_ACCESS.md' \
    "$LOCAL_DIR/" \
    "$DROPLET_USER@$DROPLET_IP:$PROJECT_DIR/"

echo ""
echo "‚úÖ Code synced successfully!"
echo ""

# Install/update dependencies on droplet
echo "üìö Installing Python dependencies on droplet..."
ssh $DROPLET_USER@$DROPLET_IP << 'ENDSSH'
    cd ~/themall/amazon-scraper

    # Activate virtual environment
    if [ ! -d "venv" ]; then
        echo "Creating virtual environment..."
        python3 -m venv venv
    fi

    source venv/bin/activate

    # Upgrade pip
    pip install --upgrade pip

    # Install requirements
    if [ -f "requirements.txt" ]; then
        echo "Installing requirements..."
        pip install -r requirements.txt
    else
        echo "‚ö†Ô∏è  No requirements.txt found!"
    fi

    # Install Playwright browsers if not exists
    if ! playwright --version &> /dev/null; then
        echo "Installing Playwright browsers..."
        playwright install chromium
    fi

    echo "‚úÖ Dependencies installed!"
ENDSSH

echo ""
echo "‚úÖ Dependencies updated successfully!"
echo ""

# Setup cron jobs
echo "‚è∞ Setting up cron jobs..."
if [ -f "$LOCAL_DIR/deploy/crontab" ]; then
    scp "$LOCAL_DIR/deploy/crontab" "$DROPLET_USER@$DROPLET_IP:/tmp/scraper_cron"
    ssh $DROPLET_USER@$DROPLET_IP "crontab /tmp/scraper_cron && rm /tmp/scraper_cron"
    echo "‚úÖ Cron jobs configured!"
else
    echo "‚ö†Ô∏è  No crontab file found, skipping cron setup"
fi

echo ""
echo "üéâ Deployment complete!"
echo ""
echo "üìã Next steps:"
echo "  1. Test scraper: ssh $DROPLET_USER@$DROPLET_IP 'cd ~/themall/amazon-scraper && source venv/bin/activate && python scripts/test_scraper.py'"
echo "  2. View cron jobs: ssh $DROPLET_USER@$DROPLET_IP 'crontab -l'"
echo "  3. Check logs: ssh $DROPLET_USER@$DROPLET_IP 'tail -f ~/themall/amazon-scraper/logs/scraper.log'"
echo ""
echo "üîó SSH: ssh $DROPLET_USER@$DROPLET_IP"
echo ""
